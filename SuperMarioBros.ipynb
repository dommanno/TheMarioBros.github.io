{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIR Project - AI Mario\n",
    "This jupyter notebook contains the application of nueral network and reinforcement learning algorithms learnt from the tutorials to simulate Mario completing a variety of levels in a Super Mario Bros pybullet gym environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mario Environment\n",
    "We use a Super Mario Bros environment (https://pypi.org/project/gym-super-mario-bros/) with a continuous state space and discrete action space. The goal of this activity is to complete Mario levels as fast as possible while also achieving a high level score. Episodes end when Mario reaches the end of the level, if Mario dies, or if a certain time as elapsed.\n",
    "\n",
    "### Action Space\n",
    "- 0: No Movement\n",
    "- 1: Move Right\n",
    "- 2: Move Right + Jump\n",
    "- 3: Move Right + Speed Up\n",
    "- 4: Move Right + Jump + Speed Up\n",
    "- 5: Jump\n",
    "- 6: Move Left\n",
    "- 7: Move Left + Jump\n",
    "- 8: Move Left + Speed Up\n",
    "- 9: Move Left + Jump + Speed Up\n",
    "- 10: Down\n",
    "- 11: Up\n",
    "\n",
    "### Observation Space\n",
    "The info dictionary returned by step contains the following:\n",
    "| Key | Unit | Description |\n",
    "| --- | ---- | ----------- |\n",
    "| coins | int | Number of collected coins |\n",
    "| flag_get | bool | True if Mario reached a flag |\n",
    "| life | int | Number of lives left |\n",
    "| score | int | Cumulative in-game score |\n",
    "| stage | int | Current stage |\n",
    "| status | str | Mario's status/power |\n",
    "| time | int | Time left on the clock |\n",
    "| world | int | Current world |\n",
    "| x_pos | int | Mario's x position in the stage |\n",
    "| y_pos | int | Mario's y position in the stage |\n",
    "\n",
    "### Rewards\n",
    "| Feature | Description | Value when Positive | Value when Negative | Value when Equal |\n",
    "|---------|-------------|---------------------|---------------------|------------------|\n",
    "| Difference in agent x values between states | Controls agent's movement | Moving right | Moving left | Not moving |\n",
    "| Time difference in the game clock between frames | Prevents agent from staying still | - | Clock ticks | Clock doesn't tick |\n",
    "| Death Penalty | Discourages agent from death | - | Agent dead | Agent alive |\n",
    "| Coins | Encourages agent to get coins | Coin collected | - | No coin collected |\n",
    "| Score | Encourages agent to get higher score | Score Value | Score Value | Score Value |\n",
    "| Flag | Encourages agent to reach middle & end flag | Flag collected | - | Flag not collected |\n",
    "| Powerup | Encourages agent to get powerups | Powerup collected | - | Powerup not collected |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:51:11\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pybullet as p\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0' \n",
    "\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "# Function to display the testing video of the agent in the juypyter notebook\n",
    "def display_video(frames, framerate=30):\n",
    "  \"\"\"Generates video from `frames`.\n",
    "\n",
    "  Args:\n",
    "    frames (ndarray): Array of shape (n_frames, height, width, 3).\n",
    "    framerate (int): Frame rate in units of Hz.\n",
    "\n",
    "  Returns:\n",
    "    Display object.\n",
    "  \"\"\"\n",
    "  height, width, _ = frames[0].shape\n",
    "  dpi = 70\n",
    "  orig_backend = matplotlib.get_backend()\n",
    "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "  ax.set_axis_off()\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_position([0, 0, 1, 1])\n",
    "  im = ax.imshow(frames[0])\n",
    "  def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "  interval = 1000/framerate\n",
    "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                  interval=interval, blit=True, repeat=False)\n",
    "  return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 2500                 # Number of episodes to train the AI on\n",
    "MEM_SIZE = 100000               # Size of the memory in replay buffer\n",
    "REPLAY_START_SIZE = 50000       # Amount of samples to fill the replay buffer before training\n",
    "MEM_RETAIN = 0.1                # Size of memory that cannot be overwritten (avoids catastrophic forgetting)\n",
    "BATCH_SIZE = 16                 # Size of random batches when sampling experiences\n",
    "LEARNING_RATE = 0.00025         # Learning rate for optimizing neural network weights\n",
    "GAMMA = 0.9                     # Discount factor for future rewards\n",
    "EPSILON_START = 1.0             # Starting exploration rate\n",
    "EPSILON_END = 0.0001            # Ending exploration rate\n",
    "EPSILON_DECAY = 2 * MEM_SIZE    # Rate at which exploration rate decays\n",
    "NETWORK_UPDATE_ITERS = 10000    # Number of iterations before learning func updates the Q weights\n",
    "MAX_STEPS = 1000                # Number of steps before the episode is terminated\n",
    "DQN_DIM1 = 256                  # Number of neurons in DQN's first hidden layer\n",
    "DQN_DIM2 = 256                  # Number of neurons in DQN's second hidden layer\n",
    "\n",
    "# Metrics for displaying training status\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_history = []\n",
    "episode_reward_history = []\n",
    "np.bool = np.bool_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Neural network class comprised of CNN and DQN to approximate Q-values for reinforcement learning\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # Constructor for Neural Network class\n",
    "    def __init__(self, env):\n",
    "        super().__init__()  # Inheriting from torch.nn.Module constructor\n",
    "\n",
    "        # Getting the input and output shapes for the neural network layers\n",
    "        self.input_shape = env.observation_space.shape\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        # Defining the convolutional layers for CNN\n",
    "        # Used for processing image data from the environment\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(self.input_shape[0], 32, kernel_size=3, stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=1, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=1, stride=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Getting the output shape of the convolutional layers\n",
    "        conv_out_size = self._get_conv_out(self.input_shape)\n",
    "\n",
    "        # Defining the layers of the Neural Network\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            self.conv_layers,\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(conv_out_size, DQN_DIM1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(DQN_DIM1, DQN_DIM2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(DQN_DIM2, self.action_space)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)  # Optimizer for the network\n",
    "        self.loss = nn.MSELoss()  # Loss function\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Device to run the network on\n",
    "        self.to(self.device)  # Moving the network to the device\n",
    "\n",
    "    # Function to get the output shape of the convolutional layers\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv_layers(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    # Foward pass through the layers of the Neural Network\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer class for storing and retrieving sampled experiences\n",
    "class ReplayBuffer:\n",
    "    # Constructor for Replay Buffer class\n",
    "    def __init__(self, env):\n",
    "        # Initialising memory count and creating arrays to store experiences\n",
    "        self.mem_count = 0\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.states = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
    "        self.actions = np.zeros(MEM_SIZE, dtype=np.int64)\n",
    "        self.rewards = np.zeros(MEM_SIZE, dtype=np.float32)\n",
    "        self.states_ = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
    "        self.dones = np.zeros(MEM_SIZE, dtype=np.bool)\n",
    "\n",
    "    # Function to add experiences to the memory buffer\n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        # If the memory count is at its max size, overwrite previous values\n",
    "        if self.mem_count < MEM_SIZE:\n",
    "            mem_index = self.mem_count  # Using mem_count if less than max memory size\n",
    "        else:\n",
    "            # Avoiding catastrophic forgetting - retain initial 10% of the replay buffer\n",
    "            mem_index = int(self.mem_count % ((1-MEM_RETAIN) * MEM_SIZE) + (MEM_RETAIN * MEM_SIZE))\n",
    "\n",
    "        # Adding the states to the replay buffer memory\n",
    "        self.states[mem_index]  = state     # Storing the state\n",
    "        self.actions[mem_index] = action    # Storing the action\n",
    "        self.rewards[mem_index] = reward    # Storing the reward\n",
    "        self.states_[mem_index] = state_    # Storing the next state\n",
    "        self.dones[mem_index] =  1 - done   # Storing the done flag\n",
    "        self.mem_count += 1  # Incrementing memory count\n",
    "\n",
    "    # Function to sample random batch of experiences\n",
    "    def sample(self):\n",
    "        MEM_MAX = min(self.mem_count, MEM_SIZE)\n",
    "        batch_indices = np.random.choice(MEM_MAX, BATCH_SIZE, replace=True).to(self.device)\n",
    "\n",
    "        states = self.states[batch_indices]\n",
    "        actions = self.actions[batch_indices]\n",
    "        rewards = self.rewards[batch_indices]\n",
    "        states_ = self.states_[batch_indices]\n",
    "        dones = self.dones[batch_indices]\n",
    "\n",
    "        # Returning the random sampled experiences\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(states_), np.array(dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.mem_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer class for storing and retrieving sampled experiences\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, env, mem_size=MEM_SIZE):\n",
    "        # Initialising memory count and creating arrays to store experiences\n",
    "        self.memory = deque(maxlen=mem_size)\n",
    "        self.mem_count = 0\n",
    "\n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        # Adding experience to memory\n",
    "        self.memory.append((state, action, reward, state_, done))\n",
    "        self.mem_count += 1\n",
    "\n",
    "    def sample(self):\n",
    "        # Randomly sample a batch of experiences\n",
    "        batch_size = min(BATCH_SIZE, self.mem_count)\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states, actions, rewards, states_, dones = zip(*batch)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(states_), np.array(dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.mem_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement Learning class\n",
    "class ReinforcementLearning:\n",
    "    # Constructor for Reinforcement Learning class\n",
    "    def __init__(self, env):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Define the device to run the network on\n",
    "        self.memory = ReplayBuffer(env)  # Creating replay buffer\n",
    "        self.policy_network = NeuralNetwork(env)  # Q\n",
    "        self.target_network = NeuralNetwork(env)  # \\hat{Q}\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())  # Initially set weights of Q to \\hat{Q}\n",
    "        self.learn_count = 0  # Tracking number of learning iterations\n",
    "\n",
    "    # Epsilon-greedy policy\n",
    "    def choose_action(self, observation):\n",
    "        # Only start decaying the epsilon once we start learning\n",
    "        if self.memory.mem_count > REPLAY_START_SIZE:\n",
    "            eps_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * \\\n",
    "                math.exp(-1. * self.learn_count / EPSILON_DECAY)\n",
    "        else:\n",
    "            eps_threshold = 1.0\n",
    "\n",
    "        # If we rolled a value lower than the epsilon sample a random action\n",
    "        if random.random() < eps_threshold:\n",
    "            return np.random.choice(np.array(range(7)), p=[0.05, 0.2, 0.2, 0.2, 0.2, 0.05, 0.1])  # Random action with set priors\n",
    "            #return np.random.choice(np.array(range(12)), p=[0.05, 0.1, 0.1, 0.1, 0.1, 0.05, 0.1, 0.1, 0.1, 0.1, 0.05, 0.05])  # Random action with set priors\n",
    "        \n",
    "        # Otherwise policy network (Q) chooses action with highest estimated Q value so far\n",
    "        state = observation.clone().detach().to(self.device)\n",
    "        state = state.unsqueeze(0)\n",
    "        self.policy_network.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_network(state)  # Get Q-values from policy network\n",
    "\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    # Main training/learning loop\n",
    "    def learn(self):\n",
    "        # Sampling a random batch of experiences and converting them to tensors\n",
    "        states, actions, rewards, states_, dones = self.memory.sample()\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        states_ = torch.tensor(states_, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(self.device)\n",
    "        batch_indices = torch.from_numpy(np.arange(BATCH_SIZE, dtype=np.int64)).to(self.device)\n",
    "\n",
    "        self.policy_network.train(True)  # Training the neural network\n",
    "        q_values = self.policy_network(states)  # Getting predicted Q-values from neural network\n",
    "        q_values = q_values[batch_indices, actions]  # Getting the Q-values for the sampled experience\n",
    "\n",
    "        self.target_network.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values_next = self.target_network(states_)  # Getting Q-values from target network\n",
    "\n",
    "        q_values_next_max = torch.max(q_values_next, dim=1)[0]  # Getting max Q-values for next state\n",
    "        q_target = rewards + GAMMA * q_values_next_max * dones  # Getting target Q-values\n",
    "\n",
    "        loss = self.policy_network.loss(q_values, q_target)  # Calcualting the loss from target and pred Q-values\n",
    "\n",
    "        # Computing the gradients and updating Q weights\n",
    "        self.policy_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.policy_network.optimizer.step()  # Updating Q weights\n",
    "        self.learn_count += 1  # Incrementing learning count\n",
    "\n",
    "        # Set target network weights to policy network weights every set increment of learning steps\n",
    "        if self.learn_count % NETWORK_UPDATE_ITERS == NETWORK_UPDATE_ITERS - 1:\n",
    "            print(\"Updating target network\")\n",
    "            self.update_target_network()\n",
    "\n",
    "    # Function to synchronize the weights of the target network with the policy network\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    # Function to return the exploration rate (epsilon) of the agent\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply additional rewards that aren't in the environment already\n",
    "def reward_shaping(prev_info, info):\n",
    "    shapedReward = 0  # Container to store the additional reward\n",
    "    reward_values = {  # Container to store keys for rewards\n",
    "        'coins': 1,\n",
    "        'score': lambda previous, current: current - previous,\n",
    "        'flag_get': 50,\n",
    "        'powerup': lambda previous, current: 1 if current > previous else 0\n",
    "    }\n",
    "\n",
    "    # Applying the reward values to the shaped reward\n",
    "    for key, reward in reward_values.items():\n",
    "        prev_value = prev_info.get(key, 0)  # Getting the previous info values for keys\n",
    "        curr_value = info.get(key, 0)       # Getting the current info values for keys\n",
    "\n",
    "        # If the reward is a function, apply the function to the previous and current values\n",
    "        if callable(reward):\n",
    "            shapedReward += reward(prev_value, curr_value)\n",
    "\n",
    "        # Otherwise, apply the reward value to the shaped reward\n",
    "        elif curr_value > prev_value:\n",
    "            shapedReward += reward\n",
    "\n",
    "    return shapedReward  # Return the shaped reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA RTX A2000 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blakemuchmorewsl/git/TheMarioBros.github.io/.venv/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/blakemuchmorewsl/git/TheMarioBros.github.io/.venv/lib/python3.8/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n",
      "/home/blakemuchmorewsl/git/TheMarioBros.github.io/.venv/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "waiting for buffer to fill...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "\n",
    "# Loading the Super Mario Bros gym environment and initialising joypad type\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', apply_api_compatibility=True, render_mode=\"rgb_array\")\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Metrics for displaying training status\n",
    "prev_info = None\n",
    "episode_reward = 0\n",
    "episode_batch_score = 0\n",
    "agent = ReinforcementLearning(env)\n",
    "plt.clf()  # Clearing previous plot\n",
    "\n",
    "env.reset()  # Reseting environment\n",
    "state_, reward, done, trunc, info = env.step(action=0)  # Taking a step in the environment\n",
    "\n",
    "# Looping through the episodes to train the model\n",
    "for episode in range(EPISODES):\n",
    "    done = False  # Setting default done state\n",
    "    step_count = 0\n",
    "    state, info = env.reset()  # Resetting environment and getting state\n",
    "    \n",
    "    # Running the episode until done or max steps reached\n",
    "    while not done:\n",
    "        # Sampling random actions and adding to the replay buffer\n",
    "        state_copy = np.array(state)\n",
    "        state_tensor = torch.tensor(state_copy, dtype=torch.float32).unsqueeze(0).squeeze(0).to(agent.device)\n",
    "        action = agent.choose_action(state_tensor)\n",
    "        state_, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Adding additional reward system\n",
    "        if prev_info is not None:\n",
    "            reward += reward_shaping(prev_info, info)\n",
    "\n",
    "        agent.memory.add(state, action, reward, state_, done)  # Add experience to replay buffer\n",
    "        # Only start learning once replay memory has reached set number of samples\n",
    "        if agent.memory.mem_count >= REPLAY_START_SIZE:\n",
    "            agent.learn()\n",
    "\n",
    "        state = state_  # Updating current state\n",
    "        prev_info = info  # Updating previous info\n",
    "        step_count += 1  # Incrementing step count\n",
    "        episode_batch_score += reward  # Updating batch reward\n",
    "        episode_reward += reward  # Updating episode reward\n",
    "\n",
    "    # Appending episode and associated reward to history\n",
    "    episode_history.append(episode)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    episode_reward = 0  # Resetting episode reward\n",
    "\n",
    "    # Printing episode number every 100 episodes\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode: \", episode)\n",
    "\n",
    "    # Saving model every batches of 100 episodes\n",
    "    if episode % 100 == 0 and agent.memory.mem_count > REPLAY_START_SIZE:\n",
    "        save_path = os.path.join(os.getcwd(), \"policy_network.pkl\")\n",
    "        torch.save(agent.policy_network.state_dict(), save_path)\n",
    "        print(\"average total reward per episode batch since episode \", episode, \": \", episode_batch_score/ float(100))\n",
    "        episode_batch_score = 0\n",
    "    elif agent.memory.mem_count < REPLAY_START_SIZE:\n",
    "        print(\"waiting for buffer to fill...\")\n",
    "        episode_batch_score = 0\n",
    "\n",
    "# Plotting the episode history and reward history\n",
    "plt.plot(episode_history, episode_reward_history)\n",
    "plt.show()\n",
    "env.close()  # Closing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Loading the Super Mario Bros gym environment and initialising joypad type\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', apply_api_compatibility=True, render_mode=\"rgb_array\")\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "agent = ReinforcementLearning(env)  # Creating reinforcement learning agent\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Defining the device to run the network on\n",
    "agent.policy_network.load_state_dict(torch.load(\"policy_network.pkl\"))  # Loading policy network\n",
    "agent.policy_network.to(device)  # Moving policy network to device\n",
    "state, info = env.reset()  # Resettin environment and getting initial state\n",
    "frames = []  # Frames container for video\n",
    "frames.append(env.render())  # Appending initial frame to video\n",
    "agent.policy_network.eval()  # Setting policy network to evaluation mode\n",
    "\n",
    "# Running the episode until done\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        state_copy = np.array(state)  # Copying state\n",
    "        state_tensor = torch.tensor(state_copy, dtype=torch.float32).unsqueeze(0).to(agent.device)  # Getting state tensor\n",
    "        q_values = agent.policy_network(state_tensor)  # Getting Q-values from policy network\n",
    "\n",
    "    # Getting the action with the highest Q-value\n",
    "    action = torch.argmax(q_values).item()\n",
    "    state, reward, done, trunc, info = env.step(action)  # Taking a step in the environment\n",
    "    frames.append(np.copy(env.render()))  # Appending frame to video\n",
    "\n",
    "    # Breaking the loop if the episode is done\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "display_video(frames)  # Displaying the video of the agent playing the game"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
