{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Guide\n",
    "For installing the Super Mario Bros gym environment package, as well as the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:51:11\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pybullet as p\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0' \n",
    "\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "# Function to display the testing video of the agent in the juypyter notebook\n",
    "def display_video(frames, framerate=30):\n",
    "  \"\"\"Generates video from `frames`.\n",
    "\n",
    "  Args:\n",
    "    frames (ndarray): Array of shape (n_frames, height, width, 3).\n",
    "    framerate (int): Frame rate in units of Hz.\n",
    "\n",
    "  Returns:\n",
    "    Display object.\n",
    "  \"\"\"\n",
    "  height, width, _ = frames[0].shape\n",
    "  dpi = 70\n",
    "  orig_backend = matplotlib.get_backend()\n",
    "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "  ax.set_axis_off()\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_position([0, 0, 1, 1])\n",
    "  im = ax.imshow(frames[0])\n",
    "  def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "  interval = 1000/framerate\n",
    "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                  interval=interval, blit=True, repeat=False)\n",
    "  return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 50000                # Number of episodes to train the AI on\n",
    "SAVE_INTERVAL = 5000            # Interval of episodes until model is saved\n",
    "MEM_SIZE = 100000               # Size of the memory in replay buffer\n",
    "REPLAY_START_SIZE = 10000       # Amount of samples to fill the replay buffer before training\n",
    "EPSILON_START = 0.1             # Starting exploration rate\n",
    "EPSILON_END = 0.0001            # Ending exploration rate\n",
    "EPSILON_DECAY = 4 * MEM_SIZE    # Rate at which exploration rate decays\n",
    "BATCH_SIZE = 32                 # Size of random batches when sampling experiences\n",
    "MEM_RETAIN = 0.1                # Size of memory that cannot be overwritten (avoids catastrophic forgetting)\n",
    "LEARNING_RATE = 0.00025         # Learning rate for optimizing neural network weights\n",
    "NETWORK_UPDATE_ITERS = 5000     # Number of iterations before learning func updates the Q weights\n",
    "GAMMA = 0.9                     # Discount factor for future rewards\n",
    "DQN_DIM1 = 256                  # Number of neurons in DQN's first hidden layer\n",
    "DQN_DIM2 = 256                  # Number of neurons in DQN's second hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Neural network class comprised of CNN and DQN to approximate Q-values for reinforcement learning\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # Constructor for Neural Network class\n",
    "    def __init__(self, env):\n",
    "        super().__init__()  # Inheriting from torch.nn.Module constructor\n",
    "\n",
    "        # Getting the input and output shapes for the neural network layers\n",
    "        self.input_shape = env.observation_shape.shape\n",
    "        self.output_shape = env.action_space.n\n",
    "\n",
    "        # Defining the layers of the Neural Network\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(*self.input_shape, DQN_DIM1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(DQN_DIM1, DQN_DIM2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(DQN_DIM2, self.output_shape)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        self.loss = nn.MSELoss()  # Loss function\n",
    "\n",
    "    # Foward pass through the layers of the Neural Network\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer class for storing and retrieving sampled experiences\n",
    "class ReplayBuffer:\n",
    "    # Constructor for Replay Buffer class\n",
    "    def __init__(self, env):\n",
    "        # Initialising memory count and creating arrays to store experiences\n",
    "        self.mem_count = 0\n",
    "        self.states = np.zeros((MEM_SIZE, *env.observation_space.shape), dtype=np.float32)\n",
    "        self.actions = np.zeros(MEM_SIZE, dtype=np.int64)\n",
    "        self.rewards = np.zeros(MEM_SIZE, dtype=np.float32)\n",
    "        self.states_ = np.zeros((MEM_SIZE, *env.observation_space.shape), dtype=np.float32)\n",
    "        self.dones = np.zeros(MEM_SIZE, dtype=np.bool)\n",
    "\n",
    "    # Function to add experiences to the memory buffer\n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        # If the memory count is at its max size, overwrite previous values\n",
    "        if self.mem_count < MEM_SIZE:\n",
    "            mem_index = self.mem_count  # Using mem_count if less than max memory size\n",
    "        else:\n",
    "            # Avoiding catastrophic forgetting - retrain initial 10% of the replay buffer\n",
    "            mem_index = int(self.mem_count % ((1-MEM_RETAIN) * MEM_SIZE) + (MEM_RETAIN * MEM_SIZE))\n",
    "\n",
    "        # Adding the states to the replay buffer memory\n",
    "        self.states[mem_index] = state\n",
    "        self.actions[mem_index] = action\n",
    "        self.rewards[mem_index] = reward\n",
    "        self.states_[mem_index] = state_\n",
    "        self.dones[mem_index] = 1 - done\n",
    "        self.mem_count += 1  # Incrementing memory count\n",
    "\n",
    "    # Function to sample random batch of experiences\n",
    "    def sample(self):\n",
    "        MEM_MAX = min(self.mem_count, MEM_SIZE)\n",
    "        batch_indices = np.random.choice(MEM_MAX, BATCH_SIZE, replace=True)\n",
    "\n",
    "        states = self.states[batch_indices]\n",
    "        actions = self.actions[batch_indices]\n",
    "        rewards = self.rewards[batch_indices]\n",
    "        states_ = self.rewards[batch_indices]\n",
    "        dones = self.dones[batch_indices]\n",
    "\n",
    "        # Returning the random sampled experiences\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement Learning class\n",
    "class ReinforcementLearning:\n",
    "    # Constructor for Reinforcement Learning class\n",
    "    def __init__(self, env):\n",
    "        self.memory = ReplayBuffer(env)  # Creating replay buffer\n",
    "        self.policy_network = NeuralNetwork(env)  # Q\n",
    "        self.target_network = NeuralNetwork(env)  # \\hat{Q}\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())  # Initially set weights of Q to \\hat{Q}\n",
    "        self.learn_count = 0  # Tracking number of learning iterations\n",
    "\n",
    "    # Epsilon-greedy policy\n",
    "    def choose_action(self, observation):\n",
    "        # Only start decaying the epsilon once we start learning\n",
    "        if self.memory.mem_count > REPLAY_START_SIZE:\n",
    "            eps_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * \\\n",
    "                math.exp(-1. * self.learn_count / EPSILON_DECAY)\n",
    "        else:\n",
    "            eps_threshold = 1.0\n",
    "\n",
    "        # If we rolled a value lower than the epsilon sample a random action\n",
    "        if random.random() < eps_threshold:\n",
    "            return np.random.choice(np.array(range(12)), p=[0.05, 0.1, 0.1, 0.1, 0.1, 0.05, 0.1, 0.1, 0.1, 0.1, 0.05, 0.05])  # Random action with set priors\n",
    "        \n",
    "        # Otherwise policy network (Q) chooses action with highest estimated Q value so far\n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        state = state.unsqueeze(0)\n",
    "        self.policy_network.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_network(state)  # Get Q-values from policy network\n",
    "\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    # Main training/learning loop\n",
    "    def learn(self):\n",
    "        # Sampling a random batch of experiences and converting them to tensors\n",
    "        states, actions, rewards, states_, dones = self.memory.sample()\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        states_ = torch.tensor(states_, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool)\n",
    "        batch_indices = np.arrange(BATCH_SIZE, dtype=np.int64)\n",
    "\n",
    "        self.policy_network.train(True)  # Training the neural network\n",
    "        q_values = self.policy_network(states)  # Getting predicted Q-values from neural network\n",
    "        q_values = q_values[batch_indices, actions]  # Getting the Q-values for the sampled experience\n",
    "\n",
    "        self.target_network.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values_next = self.target_network(states_)  # Getting Q-values from target network\n",
    "\n",
    "        q_values_next_max = torch.max(q_values_next, dim=1)[0]  # Getting max Q-values for next state\n",
    "        q_target = rewards + GAMMA * q_values_next_max * dones  # Getting target Q-values\n",
    "\n",
    "        loss = self.policy_network.loss(q_values, q_target)  # Calcualting the loss from target and pred Q-values\n",
    "\n",
    "        # Computing the gradients and updating Q weights\n",
    "        self.policy_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.policy_network.optimizer.step()  # Updating Q weights\n",
    "        self.learn_count += 1  # Incrementing learning count\n",
    "\n",
    "        # Set target network weights to policy network weights every set increment of learning steps\n",
    "        if self.learn_count % NETWORK_UPDATE_ITERS == NETWORK_UPDATE_ITERS - 1:\n",
    "            print(\"Updating target network\")\n",
    "            self.update_target_network()\n",
    "\n",
    "    # Function to synchronize the weights of the target network with the policy network\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    # Function to return the exploration rate (epsilon) of the agent\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply additional rewards that aren't in the environment already\n",
    "def reward_shaping(prev_info, info):\n",
    "    shapedReward = 0  # Container to store the additional reward\n",
    "    reward_values = {  # Container to store keys for rewards\n",
    "        'coins': 1,\n",
    "        'score': lambda previous, current: current - previous,\n",
    "        'flag_get': 50,\n",
    "        'powerup': lambda previous, current: 10 if current > previous else 0\n",
    "    }\n",
    "\n",
    "    # Applying the reward values to the shaped reward\n",
    "    for key, reward in reward_values.items():\n",
    "        prev_value = prev_info.get(key, 0)  # Getting the previous info values for keys\n",
    "        curr_value = info.get(key, 0)       # Getting the current info values for keys\n",
    "\n",
    "        # If the reward is a function, apply the function to the previous and current values\n",
    "        if callable(reward):\n",
    "            shapedReward += reward(prev_value, curr_value)\n",
    "\n",
    "        # Otherwise, apply the reward value to the shaped reward\n",
    "        elif curr_value > prev_value:\n",
    "            shapedReward += reward\n",
    "\n",
    "    return shapedReward  # Return the shaped reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "\n",
    "# Loading the Super Mario Bros gym environment and initialising joypad type\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0', render_mode='rgb', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "env.reset()  # Resetting/Activating the environment\n",
    "agent = ReinforcementLearning(env)\n",
    "plt.clf()  # Clearing previous plot\n",
    "\n",
    "# Metrics for displaying training status\n",
    "step_count = 0\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_reward = 0\n",
    "episode_batch_score = 0\n",
    "episode_history = []\n",
    "episode_reward_history = []\n",
    "np.bool = np.bool_\n",
    "\n",
    "# Looping through the episodes to train the model\n",
    "for episode in range(EPISODES):\n",
    "    done = False  # Setting default done state\n",
    "    state, info = env.reset()  # Resetting environment and getting state\n",
    "    \n",
    "    # Running the episode until done or max steps reached\n",
    "    while not done:\n",
    "        # Sampling random actions and adding to the replay buffer\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, trunc, info = env.step(action)\n",
    "        agent.memory.add(state, action, reward, state_, done)  # Add experience to replay buffer\n",
    "\n",
    "        # Only start learning once replay memory has reached set number of samples\n",
    "        if agent.memory.mem_count > REPLAY_START_SIZE:\n",
    "            agent.learn()\n",
    "\n",
    "        state = state_  # Updating current state\n",
    "        episode_batch_score += reward  # Updating batch reward\n",
    "        episode_reward += reward  # Updating episode reward\n",
    "\n",
    "    # Appending episode and associated reward to history\n",
    "    episode_history.append(episode)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    episode_reward = 0  # Resetting episode reward\n",
    "\n",
    "    # Saving model every batches of 100 episodes\n",
    "    if episode % 100 == 0 and agent.memory.mem_count > REPLAY_START_SIZE:\n",
    "        save_path = os.path.join(os.getcwd(), \"policy_network.pkl\")\n",
    "        torch.save(agent.policy_network.state_dict(), save_path)\n",
    "        print(\"average total reward per episode batch since episode \", episode, \": \", episode_batch_score/ float(100))\n",
    "        episode_batch_score = 0\n",
    "    elif agent.memory.mem_count < REPLAY_START_SIZE:\n",
    "        print(\"waiting for buffer to fill...\")\n",
    "        episode_batch_score = 0\n",
    "\n",
    "# Plotting the episode history and reward history\n",
    "plt.plot(episode_history, episode_reward_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
